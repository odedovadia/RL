defaults: base_qwen1b_squad.yaml

policy:
  dtensor_cfg:
    enabled: false
  optimizer: null
  make_sequence_length_divisible_by: ${policy.megatron_cfg.tensor_model_parallel_size}
  megatron_cfg:
    enabled: true
    converter_type: "Qwen2ForCausalLM"
    tensor_model_parallel_size: 2

logger:
  wandb:
    name: "sft-qwen1b-squad-megatron-tp2-8gpu"
  tensorboard:
    log_dir: "tb_logs-token-accuracy-megatron-tp2-8gpu"

cluster:
  gpus_per_node: 8
